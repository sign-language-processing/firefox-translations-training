disp-freq: 1000
# default learning rate for transformer-big is 0.0002 https://github.com/marian-nmt/marian-dev/blob/master/src/common/aliases.cpp
learn-rate: 0.0003 # Turn this down if you get a diverged model, maybe 0.0001
optimizer-delay: 2 # Roughly GPU devices * optimizer-delay = 8, but keep as an integer
lr-report: True
save-freq: 5000
valid-freq: 5000
valid-max-length: 300
valid-mini-batch: 8
early-stopping: 20
